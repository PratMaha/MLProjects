{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class loss(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,X_test,trees,log_threshold):\n",
    "        return 0\n",
    "\n",
    "    def g(self, y_true, pred):\n",
    "        return 0\n",
    "\n",
    "    def h(self, pred):\n",
    "        return 0\n",
    "\n",
    "class leastsquare(loss):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,X_test,trees,log_threshold):\n",
    "        predictions = []\n",
    "        #print(\"Tree Length : \" + str(len(trees)))\n",
    "        for test in X_test:\n",
    "            #count = 0\n",
    "            score = 0.0\n",
    "            for tree in trees:\n",
    "                curr_node = tree.root_node\n",
    "                while curr_node.is_leaf is not True  :\n",
    "                    curr_node = curr_node.forward(test)\n",
    "                #print(str(count) + \" \" + str(curr_node.weight))\n",
    "                #count = count + 1\n",
    "                score = score + curr_node.weight\n",
    "            score = score/len(trees)\n",
    "            predictions.append(score) \n",
    "        return predictions\n",
    "\n",
    "    def g(self, y_true, pred):\n",
    "        return (-2*(y_true - pred))\n",
    "\n",
    "    def h(self, y_true, pred):\n",
    "        return 2*np.ones(y_true.shape)\n",
    "\n",
    "class logistic(loss):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,X_test,trees,log_threshold):\n",
    "        predictions = []\n",
    "        #print(len(trees))\n",
    "        for test in X_test:\n",
    "            temp = np.zeros(len(trees))\n",
    "            for i in range(len(trees)):\n",
    "                tree = trees[i]\n",
    "                curr_node = tree.root_node\n",
    "                while(curr_node.is_leaf == False):\n",
    "                    curr_node = curr_node.forward(test)\n",
    "                temp[i] = curr_node.weight\n",
    "            min = np.min(temp)\n",
    "            max = np.max(temp)\n",
    "            #print(min)\n",
    "            #print(max)\n",
    "            if max != min :\n",
    "                temp = (temp - min)/(max - min)\n",
    "            temp = np.rint(temp)\n",
    "            unique, counts = np.unique(temp, return_counts=True)\n",
    "            #print(unique)\n",
    "            if(unique.size > 1):\n",
    "                if counts[1] > counts[0] :\n",
    "                    predictions.append(1.0)\n",
    "                else :\n",
    "                    predictions.append(0.0)\n",
    "            else :\n",
    "                predictions.append(unique[0])\n",
    "        return predictions\n",
    "\n",
    "    def g(self, y_true, pred):\n",
    "        return ((((1-y_true)*np.exp(pred)) - y_true)/(np.exp(pred) + 1))\n",
    "\n",
    "    def h(self, y_true, pred):\n",
    "        return (np.exp(pred)/((np.exp(pred) + 1)*(np.exp(pred) + 1)))*np.ones(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(self, X1, X2, index_y, threshold, value_y, is_leaf):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.left_child = X1\n",
    "        self.right_child = X2\n",
    "        self.feature_index = index_y\n",
    "        self.threshold = threshold\n",
    "        self.weight = value_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.weight)\n",
    "        #print(self.feature_index)\n",
    "        #print(self.threshold)\n",
    "        #print(self.is_leaf)\n",
    "        #print(self.left_child.is_leaf)\n",
    "        #print(self.right_child.is_leaf)\n",
    "        if self.is_leaf == True :\n",
    "            raise RuntimeError('This is a leaf node and you cannot forward from it')\n",
    "        elif x[self.feature_index] < self.threshold:\n",
    "            #print(self.left_child.is_leaf)\n",
    "            return self.left_child\n",
    "        else :\n",
    "            #print(self.right_child.is_leaf)\n",
    "            return self.right_child\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, root_node = None, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, pred = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.root_node = root_node\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.pred = pred\n",
    "        self.treenodes = []\n",
    "\n",
    "    def fit(self, X, y, loss):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        g = loss.g(y, self.pred)\n",
    "        h = loss.h(y, self.pred)\n",
    "        #print(g)\n",
    "        #print(h)\n",
    "        self.root_node = self.construct_tree(X, y, g, h, 0)\n",
    "        #print(self.root_node.weight)\n",
    "        return self\n",
    "\n",
    "    def split_dataset(self, X, y, feature_index, threshold):\n",
    "        left_mask = X[:, feature_index] < threshold\n",
    "        right_mask = X[:, feature_index] >= threshold\n",
    "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def calculate_gain(self, g_l, h_l, g_r, h_r):\n",
    "        G_l_tot = g_l.sum()\n",
    "        G_r_tot = g_r.sum()\n",
    "        H_l_tot = h_l.sum()\n",
    "        H_r_tot = h_r.sum()\n",
    "        gain = (0.5)*(((G_l_tot)*(G_l_tot)/(H_l_tot + self.lamda)) + ((G_r_tot)*(G_r_tot)/(H_r_tot + self.lamda)) - (((G_l_tot+G_r_tot)*(G_l_tot+G_r_tot))/(H_l_tot + H_r_tot+ self.lamda))) - self.gamma\n",
    "        return gain\n",
    "        \n",
    "    def find_threshold(self, g, h, X, y, feature_index):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        best_gain = 0\n",
    "        best_threshold = 0\n",
    "        thresholds = np.unique(X[:, feature_index])\n",
    "        for threshold in thresholds:\n",
    "            X_left, X_right, y_left, y_right = self.split_dataset(X, y, feature_index, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "            left_mask = X[:, feature_index] < threshold\n",
    "            right_mask = X[:, feature_index] >= threshold\n",
    "            gain = self.calculate_gain(g[left_mask], h[left_mask], g[right_mask], h[right_mask])\n",
    "            #print(\"Gain : \" +str(gain))\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_threshold = threshold\n",
    "        return [threshold, best_gain]\n",
    "        \n",
    "    def construct_tree(self, X, y, g, h, current_depth):\n",
    "        '''\n",
    "        Node Addition, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        if current_depth > self.max_depth or y.size < self.min_sample_split :\n",
    "            return None\n",
    "        best_feature, threshold, best_gain = self.find_best_decision_rule(X, y, g, h)\n",
    "        #print(\"best_feature : \" + str(best_feature))\n",
    "        #print(\"threshold : \" + str(threshold))\n",
    "        #print(\"best_gain : \" + str(best_gain))\n",
    "        if best_gain == 0 : \n",
    "            return None\n",
    "        #print(X[:, best_feature])\n",
    "        left_mask = X[:, best_feature] < threshold\n",
    "        right_mask = X[:, best_feature] >= threshold\n",
    "        node_left = self.construct_tree(X[left_mask], y[left_mask], g[left_mask], h[left_mask], current_depth + 1)\n",
    "        node_right = self.construct_tree(X[right_mask], y[right_mask], g[right_mask], h[right_mask], current_depth + 1)\n",
    "        h_val = h.sum()\n",
    "        weight = -(g.sum()/(h_val + self.lamda))\n",
    "        is_leaf = False\n",
    "        if node_left is None and node_right is not None :\n",
    "            node_left = TreeNode(None, None, 0, 0, weight, True)\n",
    "        elif node_right is None and node_left is not None :\n",
    "            node_right = TreeNode(None, None, 0, 0, weight, True)\n",
    "        elif node_right is None and node_left is None :\n",
    "            is_leaf = True\n",
    "        node = TreeNode(node_left, node_right, best_feature, threshold, weight, is_leaf)\n",
    "        self.treenodes.append(node)\n",
    "        #print(node)\n",
    "        return node\n",
    "\n",
    "    def find_best_decision_rule(self, X, y, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        best_gain = 0\n",
    "        best_threshold = 0 \n",
    "        best_feature = 0\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            #print(\"Feature \" + str(feature_index))\n",
    "            threshold, gain = self.find_threshold(g, h, X, y, feature_index)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_threshold = threshold\n",
    "                best_feature = feature_index\n",
    "        return best_feature, threshold, best_gain\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,log_threshold = 0.5,\n",
    "        n_threads = 0, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        self.log_threshold = log_threshold\n",
    "        self.n_threads = n_threads\n",
    "        if loss == 'mse':\n",
    "            self.loss = leastsquare()\n",
    "        else : \n",
    "            self.loss = logistic()\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X is n x m 2d numpy array\n",
    "        # y is n-dim 1d array\n",
    "        num_samples, num_features = X.shape\n",
    "        for i in range(self.num_trees):\n",
    "            #print (\"Tree : \" + str(i))\n",
    "            m_rf = (int)(np.ceil(self.rf*num_features))\n",
    "            m_rf_indices = np.random.choice(num_features, m_rf, replace=False)\n",
    "            n_rf_indices = np.random.choice(num_samples, num_samples, replace=True)\n",
    "            XTree = X[n_rf_indices][:,m_rf_indices]\n",
    "            #print(XTree.shape)\n",
    "            yTree = y[n_rf_indices]\n",
    "            prediction = 0\n",
    "            tree = Tree(None, self.n_threads, self.max_depth, self.min_sample_split,self.lamda, self.gamma, prediction)\n",
    "            tree = tree.fit(XTree, yTree, self.loss)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    # test is the input to the model\n",
    "    def predict(self, X_test):\n",
    "        return self.loss.pred(X_test, self.trees, self.log_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self, log_threshold = 0.5,\n",
    "        learning_rate=0.5, n_threads = 0, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        self.loss_type = loss\n",
    "        self.n_threads = n_threads\n",
    "        if loss == 'mse':\n",
    "            self.loss = leastsquare()\n",
    "        else : \n",
    "            self.loss = logistic()\n",
    "        self.max_depth = max_depth\n",
    "        self.log_threshold = log_threshold\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X is n x m 2d numpy array\n",
    "        # y is n-dim 1d array\n",
    "        num_samples, num_features = X.shape\n",
    "        prediction = np.zeros(y.shape)\n",
    "        for i in range(self.num_trees):\n",
    "            #print (\"Tree : \" + str(i))\n",
    "            tree = Tree(None, self.n_threads, self.max_depth, self.min_sample_split,self.lamda, self.gamma, prediction)\n",
    "            tree = tree.fit(X, y, self.loss)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            if self.loss_type == 'mse':\n",
    "                new_pred =self.loss.pred(X, [tree], self.log_threshold)\n",
    "            else :\n",
    "                count0 = 0\n",
    "                count1 = 1\n",
    "                curr_node = tree.root_node\n",
    "                new_pred = []\n",
    "                for test in X:\n",
    "                    while(curr_node.is_leaf == False):\n",
    "                        curr_node = curr_node.forward(test)\n",
    "                    if curr_node.weight > self.log_threshold:\n",
    "                        count1 = count1 + 1\n",
    "                    else :\n",
    "                        count0 = count0 + 1\n",
    "                    if count1 > count0 :\n",
    "                        new_pred.append(1.0)\n",
    "                    else :\n",
    "                        new_pred.append(0.0)\n",
    "            prediction = self.learning_rate*prediction + np.array(new_pred)\n",
    "        return self\n",
    "\n",
    "    # test is the input to the model\n",
    "    def predict(self, X_test):\n",
    "        return self.loss.pred(X_test, self.trees, self.log_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    diff_matrix = y - pred\n",
    "    #print(y)\n",
    "    #print(diff_matrix)\n",
    "    rmse = diff_matrix**2\n",
    "    rmse = rmse.sum()\n",
    "    rmse = rmse/np.size(y)\n",
    "    rmse = np.sqrt(rmse)\n",
    "    return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    dataset_size = y.size\n",
    "    correct_predictions = 0\n",
    "    #print(pred)\n",
    "    #print(y)\n",
    "    for i in range(dataset_size):\n",
    "        if y[i] == (pred[i]):\n",
    "            correct_predictions += 1\n",
    "    precision = (correct_predictions/dataset_size)*100\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prateek/anaconda3/envs/AnacondaTest/lib/python3.11/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE of the Random Forest Method for the Boston Dataset is :  9.229962989283528\n",
      "Testing RMSE of the Random Forest Method for the Boston Dataset is :  8.975222658557659\n",
      "Training Set RMSE of Linear Regression for Boston Dataset (from HW2): 4.820626531838223\n",
      "Testing Set RMSE of Linear Regression for Boston Dataset (from HW2): 5.209217510530916\n",
      "Training Set RMSE of Ridge Regression for Boston Dataset (from HW2): 4.829777333975097\n",
      "Testing Set RMSE of Ridge Regression for Boston Dataset (from HW2): 5.189347305423606\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest Code\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "RF_regression = RF(0.5, n_threads = 0, loss = 'mse', max_depth = 8, min_sample_split = 10, lamda = 1, gamma = 1,rf = 0.8, num_trees = 99)\n",
    "RF_regression = RF_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = RF_regression.predict(X_train)\n",
    "pred_test = RF_regression.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "print(\"Training RMSE of the Random Forest Method for the Boston Dataset is : \", rmse_train)\n",
    "print(\"Testing RMSE of the Random Forest Method for the Boston Dataset is : \", rmse_test)\n",
    "print(\"Training Set RMSE of Linear Regression for Boston Dataset (from HW2): 4.820626531838223\")\n",
    "print(\"Testing Set RMSE of Linear Regression for Boston Dataset (from HW2): 5.209217510530916\")\n",
    "print(\"Training Set RMSE of Ridge Regression for Boston Dataset (from HW2): 4.829777333975097\")\n",
    "print(\"Testing Set RMSE of Ridge Regression for Boston Dataset (from HW2): 5.189347305423606\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the Random Forest Method for the Credit-g Dataset is :  69.71428571428572\n",
      "Testing Accuracy of the Random Forest Method for the Credit-g Dataset is :  70.0\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/',as_frame=False)\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "#print(X_train)\n",
    "RF_regression = RF(log_threshold = 0.5,n_threads = 0, loss = 'class', max_depth = 8, min_sample_split = 10, lamda = 1, gamma = 1,rf = 0.5, num_trees = 99)\n",
    "RF_regression = RF_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = RF_regression.predict(X_train)\n",
    "pred_test = RF_regression.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "print(\"Training Accuracy of the Random Forest Method for the Credit-g Dataset is : \", acc_train)\n",
    "print(\"Testing Accuracy of the Random Forest Method for the Credit-g Dataset is : \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the Random Forest Method for the Breast Cancer Dataset is :  63.31658291457286\n",
      "Testing Accuracy of the Random Forest Method for the Breast Cancer Dataset is :  61.40350877192983\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "RF_regression = RF(log_threshold = 0.5, n_threads = 0, loss = 'class', max_depth = 10, min_sample_split = 10, lamda = 1, gamma = 1,rf = 0.5, num_trees = 99)\n",
    "RF_regression = RF_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = RF_regression.predict(X_train)\n",
    "pred_test = RF_regression.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "print(\"Training Accuracy of the Random Forest Method for the Breast Cancer Dataset is : \", acc_train)\n",
    "print(\"Testing Accuracy of the Random Forest Method for the Breast Cancer Dataset is : \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE of the GBDT Method for the Boston Dataset is :  15.3444415615304\n",
      "Testing RMSE of the GBDT Method for the Boston Dataset is :  15.524724610785293\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "learning_rate = 0.2\n",
    "GBDT_regression = GBDT(0.5, learning_rate, n_threads = 0, loss = 'mse', max_depth = 8, min_sample_split = 10, lamda = 1, gamma = 1,rf = 0.5, num_trees = 99)\n",
    "GBDT_regression = GBDT_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = GBDT_regression.predict(X_train)\n",
    "pred_test = GBDT_regression.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "print(\"Training RMSE of the GBDT Method for the Boston Dataset is : \", rmse_train)\n",
    "print(\"Testing RMSE of the GBDT Method for the Boston Dataset is : \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the GBDT Method for the Credit-g Dataset is :  68.85714285714286\n",
      "Testing Accuracy of the GBDT Method for the Credit-g Dataset is :  71.33333333333334\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/',as_frame=False)\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "learning_rate = 0.9\n",
    "GBDT_regression = GBDT(0.5, learning_rate, n_threads = 0, loss = 'class', max_depth = 8, min_sample_split = 10, lamda = 5, gamma = 0.5,rf = 0.5, num_trees = 100)\n",
    "GBDT_regression = GBDT_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = GBDT_regression.predict(X_train)\n",
    "pred_test = GBDT_regression.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "print(\"Training Accuracy of the GBDT Method for the Credit-g Dataset is : \", acc_train)\n",
    "print(\"Testing Accuracy of the GBDT Method for the Credit-g Dataset is : \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy of the GBDT Method for the Breast Cancer Dataset is :  63.31658291457286\n",
      "Testing Accuracy of the GBDT Method for the Breast Cancer Dataset is :  61.40350877192983\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "learning_rate = 0.8\n",
    "GBDT_regression = GBDT(0.5, learning_rate, n_threads = 0, loss = 'class', max_depth = 4, min_sample_split = 10, lamda = 6, gamma = 0.8,rf = 0.5, num_trees = 60)\n",
    "GBDT_regression = GBDT_regression.fit(np.array(X_train), np.array(y_train))\n",
    "pred_train = GBDT_regression.predict(X_train)\n",
    "pred_test = GBDT_regression.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "print(\"Training Accuracy of the GBDT Method for the Breast Cancer Dataset is : \", acc_train)\n",
    "print(\"Testing Accuracy of the GBDT Method for the Breast Cancer Dataset is : \", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
